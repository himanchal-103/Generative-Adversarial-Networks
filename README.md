# Generative-Adversarial-Networks

## Overview
Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed for generating synthetic data that resembles real-world data. GANs were introduced by Ian Goodfellow and his colleagues in 2014. The core idea behind GANs is to train two neural networks, the Generator and the Discriminator, in a competitive setting where they are trained simultaneously and have opposing objectives.

### Objectives
1. Generator Objective: Maximize the probability of the Discriminator making a mistake.
2. Discriminator Objective: Maximize the probability of correctly identifying real vs. fake data.

### Result
1. Start of the training
<img width="517" alt="Screenshot 2024-08-13 at 7 30 40 PM" src="https://github.com/user-attachments/assets/a82e720b-bc2b-41e8-b03d-686a4103b100">

2. End of the training
<img width="517" alt="Screenshot 2024-08-13 at 7 31 17 PM" src="https://github.com/user-attachments/assets/682e5d03-d7b6-4a04-ae92-fb45747bdd58">


## Components

### Generator
1. Purpose: To create synthetic data that mimics real data.
2. Function: Takes random noise as input and generates data (e.g., images) as output.
3. Objective: To improve in creating data that can fool the Discriminator into believing it's real.

### Discriminator
1. Purpose: To distinguish between real data and synthetic data.
2. Function: Takes data (both real and generated) as input and outputs a probability indicating whether the data is real or fake.
3. Objective: To improve in accurately identifying whether the data is from the real dataset or generated by the Generator.

### Training Process
1. Initialization: Both the Generator and Discriminator networks are initialized with random weights.
2. Data Generation: The Generator creates synthetic data from random noise.
3. Discriminator Evaluation: The Discriminator evaluates both real data and synthetic data, providing feedback.
4. Loss Calculation: The losses for both the Generator and Discriminator are computed based on their respective objectives.
5. Backpropagation: The networks are updated based on the computed losses to improve their performance.
6. Iteration: Steps 2-5 are repeated iteratively until the Generator produces data that is indistinguishable from real data by the Discriminator.

### Applications
1. Image Generation: Creating realistic images from noise or textual descriptions.
2. Data Augmentation: Generating additional training samples for machine learning models.
3. Style Transfer: Applying the style of one image to another while preserving the original content.
4. Art Creation: Generating artwork or creative content using trained GANs.

### Popular GAN Architectures
1. DCGAN (Deep Convolutional GAN): Uses convolutional layers in both Generator and Discriminator to generate high-quality images.
2. WGAN (Wasserstein GAN): Improves training stability using the Wasserstein loss function.
3. CycleGAN: Performs image-to-image translation tasks without requiring paired examples.
